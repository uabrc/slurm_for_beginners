{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SLURM for Beginners\n",
    "Slurm is software that gives researchers fair allocation of the cluster's resources. It schedules jobs based on the resource requests ( e.g CPUs, RAM, run time, and etc).\n",
    "\n",
    "### Running SLURM jobs on Cheaha\n",
    "\n",
    "All work on Cheaha must be submitted to the queueing system, **Slurm**. The purpose of this tutorial is to gives a basic overview of Slurm and guide beginners on how to effectively submit, monitor, and manage jobs on a SLURM-based system. All scripts in this Jupyter Notebook are developed for the **Cheaha**, the UABRC system, so some part of codes/texts may be specific to our setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Cheaha, you can execute your job in two ways:\n",
    "- **Interactive jobs**: using OOD sessions e.g Desktop, Jupyter Notebook\n",
    "- **Batch jobs**: can be submitted to the SLURM workload manager using `SBATCH` ( the focus of this tutorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute a batch job, we use two files:\n",
    "- the job script ( the actual task script e.g python scripts  with file extensions `.py`)\n",
    "- the job submission script (`SBATCH` file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Job script (`print_array.py`)**\n",
    "\n",
    "Below is the actual job script that prints a simple numpy array and if we run this code block this scipt wil be saved in the same directory as this jupyter notebook as `print_array.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat <<EOT > \"print_array.py\"\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import pylab\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "a = np.array([0, 10, 20, 30, 40])\n",
    "print(a)\n",
    "\n",
    "b = np.arange(-5, 5, 0.5)\n",
    "print(b)\n",
    "\n",
    "plt.plot([1, 2, 3, 4], [1, 4, 9, 16])\n",
    "plt.show()\n",
    "plt.savefig('testing.png')\n",
    "EOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The cells in this Jupyter notebook uses the ipython magic `%%bash` to run the contents of the cell using the bash shell, as though typed at a terminal. When you plan to write your own code you just ignore these two lines at the begining and the `EOT` at the end. Refer [here](https://github.com/uabrc/How-To-Slurm/blob/main/slurm_how_to.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat \"print_array.py\" # just to check if these lines of code are saved in the `print_array.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Job submission script (`submit_job_array.sh`)**\n",
    "\n",
    "Below is the job submission script `submit_job_array.sh`, which consists of the a SLURM-based systems job excution stpes:\n",
    "- Resource request for the job (lines that start with `#SBATCH`)\n",
    "- Define Job steps:\n",
    "    - Defines the environment (lines that start with `module` and `conda`)\n",
    "    - Describe the actual job script (a python script `print_array.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat <<EOT > \"submit_job_array.sh\"\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=print_array_job       \n",
    "#SBATCH --output=%x_%j.out           \n",
    "#SBATCH --error=%x_%j.err            \n",
    "#SBATCH --partition=amd-hdr100       \n",
    "#SBATCH --nodes=1                   \n",
    "#SBATCH --ntasks=1                   \n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=2G\n",
    "#SBATCH --time=00:20:00\n",
    "\n",
    "# Load necessary modules\n",
    "module reset\n",
    "module load Miniforge3\n",
    "conda activate tf_gpu\n",
    "# Run your Python script\n",
    "python print_array.py\n",
    "EOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat \"submit_job_array.sh\" # view the submission script written to the the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job  submission \n",
    "\n",
    " Once the actual script and the job submission script are ready, then we can submit the job and monitor the progress of our job. Once we submitted the job, SLURM will first check the job submission script to determine the requested resources and place it on the job queue. Once the resources become available, then the job is executed. See the below examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next line to submit the job script to the scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sbatch submit_job_array.sh #submit the job that we defined above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring the status of the job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the status of your job by using the `squeue` command. The  `squeue` normally used to montior jobs that are currently running and it lists all jobs for a specific researcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!squeue -u $USER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the output will show information about the jobs that are include `jobid` as the ID of the job asssigned by the SLURM scheduler,  `partition`, `jobname` as `name`, `blazerid` as `user`, job state as `st`, total run time as `time`, number of nodes as `node`, and the list of nodes as `nodelist` used for each job a researcher has submitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once submitting the job, Slurm will create the output and error files with job name `even_job` and a jobid assigned by the Slurm scheduler (since `even_job` is a job name for this specific example), you can run the `ls` command as given below and see the results. Normally, the ls command will list all files in the directory where we are currently running our job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls # to list files in our directory, and check the slurm output file for results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the job you submitted has finished running, use the following two command to check the output and error in the job's log file respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat \"print_array_job_27728525.out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat \"print_array_job_27728525.err\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review results\n",
    "\n",
    "We can use the `sacct` command to review our previous just for better understanding your own usage history. To read more on `sacct` and reterive customized outputs, please refer our [documentation](https://docs.rc.uab.edu/cheaha/slurm/job_management/#review-jobs-by-jobid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sacct -j [jobid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore resource usage efficiency\n",
    "\n",
    "Efficient jobs save you time. Optimizing queue wait times relies on getting resource requests close to actual resource usage. It is important to evaluate the efficiency of your job in terms of resource usage after it completes. To do so, use `seff`, a **S**lurm **Eff**iciency tool for exploring resource usage efficiency.\n",
    "\n",
    "Remember that Cheaha is a shared resource, so requesting resources that sit unused during a job prevents others from using those resources. It is recommended to aim for memory efficiency between 75% and 90%. Utilizing less than 75% may waste resources, while approaching 100% could risk job failure due to out-of-memory issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! seff [jobid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: for running and failed jobs, the efficiency numbers reported by `seff` are not reliable. This tool is usefull only for successfully **completed** jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing existing job in the queue\n",
    "\n",
    "Cancelling queued and currently running jobs can be done using the `scancel` command. Importantly, this action will only cancel jobs that were initiated by the researcher running the command. You can run the following commands as per you goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `scancel` command followed by the 'jobID' to cancel **either a single job** or **an entire job array**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scancel [jobid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to cancel specific job array, whether it's a single number or a range, you can use the following command. Simply replace `[jobid_arrayid]` with the respective `jobID` or `arrayID`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scancel [jobid_arrayid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `scancel` command followed by 'partition' name to cancel **all jobs on a partition for the user**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scancel -p [partition]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `scancel` command followed by 'user's name to cancel **all jobs for a researcher**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scancel -u $USER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting  Jupyter Notebooks with SLURM's  `sbatch` command on Cheaha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will cover how to execute Jupyter Notebook files (`.ipynb`) using Slurm's `sbatch` command. This allows you to submit Jupyter Notebooks as batch jobs to the Slurm scheduler for execution.\n",
    "\n",
    "You can follow these steps:\n",
    "\n",
    "1. Prepare the input Notebook. Assume, you have a notebook file (`data_processing.ipynb`) containing the code to be executed.\n",
    "\n",
    "1. Create the below job submission script and save it as `submit_job.sh` for this example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH --job-name=data_processing       \n",
    "#SBATCH --output=%x_%j.out           \n",
    "#SBATCH --error=%x_%j.err            \n",
    "#SBATCH --partition=amd-hdr100       \n",
    "#SBATCH --nodes=1                   \n",
    "#SBATCH --ntasks=1                   \n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=2G\n",
    "#SBATCH --time=00:20:00\n",
    "\n",
    "# load necessary modules\n",
    "module reset\n",
    "module load Miniforge3\n",
    "conda activate tf_gpu\n",
    "#by default, Jupyter Notebook doesn't redirect output to stdout. \n",
    "# Only errors and exceptions are typically captured. So, `--output=%x_%j.out`  will be empty\n",
    "#cd /path/to/notebook\n",
    "jupyter nbconvert --to notebook --execute data_processing.ipynb --output data_processing_output.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The command `jupyter nbconvert --to notebook --execute data_processing.ipynb --output data_processing_output.ipynb`, in the above job submission script, executes the code cells in the `data_processing.ipynb` notebook file and saves the output to `data_processing_output.ipynb`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use the `sbatch` command to submit the job to the Slurm scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sbatch submit_job.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. You can monitor the status of this job using `squeue` or `sacct` commands, as shown in the examples above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Once the job completes successfully, you can browse the output notebook file named `data_processing_output.ipynb` in the same directory where the job was submitted. Open the output notebook using Jupyter Notebook and then you should view the executed code and its outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requesting GPUs for submitting GPU-Accelerated jobs with Slurm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section provides a comprehensive guide on submitting GPU-accelerated jobs to the Cheaha cluster using Slurm. Without explicit specification, jobs won't be allocated GPUs. \n",
    "\n",
    "To request GPUs, include the `--gres` option in your `sbatch script`, specifying the number of GPUs required. Also, ensure that you select the partition that has GPU resources available. For example, partitions `pascalnodes`( P100 GPUs) or `amperenodes` (A100 GPUs) family are typically the only partitions that have GPU resources available on the Cheaha clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH ...\n",
    "#SBATCH --partition=pascalnodes\n",
    "#SBATCH --gres=gpu:2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script requests 2 GPU using the `--gres` option and specifies the `pascalnodes` partition with GPU resources using the `--partition` option. You can replace 1 with the desired number of GPUs, and `pascalnodes` with the other partition name (for example with `amperenodes`) available on Cheaha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if you would like to run the script (`print_array.py`) using a GPU, you should create a job submission script as shown below. Note that the CUDA module is loaded for GPU support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat <<EOT > \"submit_job_array.sh\"\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=print_array_job       \n",
    "#SBATCH --output=%x_%j.out           \n",
    "#SBATCH --error=%x_%j.err            \n",
    "#SBATCH --partition=amd-hdr100       \n",
    "#SBATCH --nodes=1                   \n",
    "#SBATCH --ntasks=1                   \n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=2G\n",
    "#SBATCH --partition=amperenodes \n",
    "#SBATCH --gres=gpu:2               \n",
    "#SBATCH --time=00:20:00\n",
    "\n",
    "# Load necessary modules\n",
    "module reset\n",
    "### Loading the required CUDA and cuDNN modules\n",
    "module load CUDA/12.2.0\n",
    "module load cuDNN/8.9.2.26-CUDA-12.2.0\n",
    "\n",
    "### loading the miniforge module and activating the environment\n",
    "module load Miniforge3\n",
    "conda activate tf_gpu\n",
    "# Run your Python script\n",
    "python print_array.py\n",
    "EOT"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
